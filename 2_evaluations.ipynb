{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining BM25 top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytrec_eval\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "\n",
    "def get_bm25_run(top_k: int, run_file: str = \"run.miracl.bm25.fr.dev.txt\") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Reads BM25 run file and returns top-k results for each query\n",
    "    \n",
    "    Args:\n",
    "        top_k: Number of top documents to return per query\n",
    "        run_file: Path to the BM25 run file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping query IDs to list of (doc_id, score) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    if top_k > 5000:\n",
    "        cmd = \"\"\"\n",
    "        python -m pyserini.search.lucene \\\n",
    "            --threads 16 --batch-size 128 \\\n",
    "            --language fr \\\n",
    "            --topics miracl-v1.0-fr-dev \\\n",
    "            --index miracl-v1.0-fr \\\n",
    "            --output run.miracl.bm25.fr.dev.txt \\\n",
    "            --bm25 --hits 5000\n",
    "        \"\"\"\n",
    "        raise Exception(f\"I've saved top 5k results in the run file. You may need to rerun the search with {cmd}\")\n",
    "\n",
    "    runs = {}\n",
    "    with open(run_file, 'r') as f:\n",
    "        for line in f:\n",
    "            qid, _, docid, rank, score, _ = line.strip().split()\n",
    "            if qid not in runs:\n",
    "                runs[qid] = []\n",
    "            if len(runs[qid]) < top_k:\n",
    "                runs[qid].append((docid, float(score)))\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Tuple\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _get_split_data(miracl_ds: datasets.DatasetDict, \n",
    "                   split: str) -> Tuple[Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Extracts queries and relevance judgments for a specific split\n",
    "    \n",
    "    Args:\n",
    "        miracl_ds: MIRACL dataset dictionary\n",
    "        split: Split name ('train' or 'dev')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (queries dict, qrels dict)\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    qrels = {}\n",
    "    \n",
    "    if split in miracl_ds:\n",
    "        for item in miracl_ds[split]:\n",
    "            qid = item['query_id']\n",
    "            queries[qid] = item['query']\n",
    "            \n",
    "            if 'positive_passages' in item:\n",
    "                qrels[qid] = {\n",
    "                    passage['docid']: 1 \n",
    "                    for passage in item['positive_passages']\n",
    "                }\n",
    "    \n",
    "    return queries, qrels\n",
    "\n",
    "def load_miracl_split(lang: str = 'fr', \n",
    "                     split: str = 'dev',\n",
    "                     cache_dir: str = \"hf_datasets_cache\") -> Tuple[Dict[str, str], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Loads MIRACL data for a specific split\n",
    "    \n",
    "    Args:\n",
    "        lang: Language code\n",
    "        split: Split to load ('train' or 'dev')\n",
    "        cache_dir: Cache directory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (documents dict, queries dict, qrels dict)\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    miracl_ds = datasets.load_dataset(\"miracl\", lang, cache_dir=cache_dir)\n",
    "    collection_ds = datasets.load_dataset(\"miracl/miracl-corpus\", lang, cache_dir=cache_dir)\n",
    "    \n",
    "    # Convert collection to dictionary\n",
    "    documents = {\n",
    "        doc['docid']: (doc.get('title', '') + \" \" + doc['text']).strip()\n",
    "        for doc in collection_ds['train']\n",
    "    }\n",
    "    \n",
    "    # Get queries and qrels for specific split\n",
    "    queries, qrels = _get_split_data(miracl_ds, split)\n",
    "    \n",
    "    return documents, queries, qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "\n",
    "def save_runs(runs: Dict[str, List[Tuple[str, float]]], \n",
    "             output_path: str,\n",
    "             run_name: str = \"miracl\",\n",
    "             max_rank: int = 1000) -> None:\n",
    "    \"\"\"\n",
    "    Saves retrieval/reranking results in TREC format\n",
    "    \n",
    "    Args:\n",
    "        runs: Dictionary mapping query IDs to lists of (doc_id, score) tuples\n",
    "        output_path: Path to save the run file\n",
    "        run_name: Name of the run (used in the output format)\n",
    "        max_rank: Maximum number of results to save per query\n",
    "    \n",
    "    The TREC format is:\n",
    "    qid Q0 docid rank score run_name\n",
    "    \n",
    "    Example:\n",
    "    1 Q0 doc1 1 14.8989 miracl\n",
    "    1 Q0 doc2 2 14.7654 miracl\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving runs to {output_path}\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for qid, doc_scores in runs.items():\n",
    "            # Sort by score in descending order if not already sorted\n",
    "            sorted_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Write top max_rank results\n",
    "            for rank, (doc_id, score) in enumerate(sorted_docs[:max_rank], start=1):\n",
    "                # TREC format: qid Q0 docid rank score run_name\n",
    "                f.write(f\"{qid} Q0 {doc_id} {rank} {score:.6f} {run_name}\\n\")\n",
    "    \n",
    "    print(f\"Saved results for {len(runs)} queries\")\n",
    "\n",
    "# Example usage with validation\n",
    "def save_runs_with_validation(runs: Dict[str, List[Tuple[str, float]]], \n",
    "                            output_path: str,\n",
    "                            run_name: str = \"miracl\",\n",
    "                            max_rank: int = 1000) -> None:\n",
    "    \"\"\"\n",
    "    Saves runs with additional validation checks\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not runs:\n",
    "        raise ValueError(\"Empty runs dictionary provided\")\n",
    "    \n",
    "    # Check if all queries have results\n",
    "    empty_queries = [qid for qid, docs in runs.items() if not docs]\n",
    "    if empty_queries:\n",
    "        print(f\"Warning: {len(empty_queries)} queries have no results\")\n",
    "    \n",
    "    # Check for reasonable score ranges\n",
    "    for qid, doc_scores in runs.items():\n",
    "        scores = [score for _, score in doc_scores]\n",
    "        if scores:\n",
    "            min_score, max_score = min(scores), max(scores)\n",
    "            if max_score > 1e6 or min_score < -1e6:\n",
    "                print(f\"Warning: Unusual score range for query {qid}: [{min_score}, {max_score}]\")\n",
    "    \n",
    "    # Save runs\n",
    "    save_runs(runs, output_path, run_name, max_rank)\n",
    "    \n",
    "    # Verify file was created and is non-empty\n",
    "    if not os.path.exists(output_path):\n",
    "        raise RuntimeError(f\"Failed to create output file: {output_path}\")\n",
    "    \n",
    "    if os.path.getsize(output_path) == 0:\n",
    "        raise RuntimeError(f\"Output file is empty: {output_path}\")\n",
    "    \n",
    "    # Print first few lines of the file for verification\n",
    "    print(\"\\nFirst few lines of the output file:\")\n",
    "    with open(output_path, 'r') as f:\n",
    "        for _ in range(3):\n",
    "            line = f.readline().strip()\n",
    "            if line:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common 'Reranker' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "\n",
    "class Reranker:\n",
    "    def __init__(self, model_name: str, batch_size: int = 8):\n",
    "        is_mps = torch.backends.mps.is_available()\n",
    "        is_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        self.device = torch.device('mps' if is_mps else 'cuda' if is_cuda else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if 'crossencoder' in model_name.lower():\n",
    "            logging.info(\"Actually using a CrossEncoder this time lmaoo\")\n",
    "            self.model = CrossEncoder(model_name, num_labels=1)\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "    def rerank_batch(self, query: str, documents: List[str], \n",
    "                    doc_ids: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Rerank a batch of documents\"\"\"\n",
    "        if isinstance(self.model, CrossEncoder):\n",
    "            pairs = [[query, doc] for doc in documents]\n",
    "            scores = self.model.predict(pairs, show_progress_bar=False)\n",
    "        else:\n",
    "            query_emb = self.model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "            doc_embs = self.model.encode(documents, convert_to_tensor=True, show_progress_bar=False)\n",
    "            scores = util.pytorch_cos_sim(query_emb, doc_embs)[0].cpu().numpy()\n",
    "        \n",
    "        return list(zip(doc_ids, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a reranking pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    dataset = datasets.load_dataset(\"miracl/miracl-corpus\", \"fr\", cache_dir=\"hf_datasets_cache\")['train']\n",
    "    return dataset.num_rows\n",
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentStore:\n",
    "    \"\"\"Memory-efficient document store that loads documents on demand\"\"\"\n",
    "    def __init__(self, lang: str, doc_ids: list[str], cache_dir: str = \"hf_datasets_cache\"):\n",
    "        dataset = datasets.load_dataset(\"miracl/miracl-corpus\"), lang, cache_dir=cache_dir)['train']\n",
    "\n",
    "        # Convert collection to dictionary\n",
    "        logging.info(f\"\\nLoading {len(doc_ids)}/14636953 documents into memory (dict)..\")\n",
    "        self.documents = {\n",
    "            doc['docid']: (doc.get('title', '') + \" \" + doc['text']).strip()\n",
    "            for doc in dataset if doc['docid'] in doc_ids\n",
    "        }\n",
    "        logging.info(f\"\\n DONE loading all documents into memory (dict).\")\n",
    "    \n",
    "    def get_documents(self, doc_ids: List[str]) -> List[str]:\n",
    "        \"\"\"Fetch documents by ID\"\"\"\n",
    "        documents = []\n",
    "        for doc_id in doc_ids:\n",
    "            documents.append(self.documents[doc_id])\n",
    "        return documents\n",
    "\n",
    "class QueryStore:\n",
    "    \"\"\"Memory-efficient query store that loads queries on demand\"\"\"\n",
    "    def __init__(self, lang: str, split: str = 'dev', cache_dir: str = \"hf_datasets_cache\"):\n",
    "        self.dataset = datasets.load_dataset(\"miracl/miracl\", lang, cache_dir=cache_dir)[split]\n",
    "        self._query_map = {item['query_id']: item['query'] for item in self.dataset}\n",
    "    \n",
    "    def get_query(self, query_id: str) -> str:\n",
    "        return self._query_map.get(query_id, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def batch_iterator(items: List, batch_size: int) -> Iterator:\n",
    "    \"\"\"Yield items in batches\"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "def rerank_runs(initial_runs: Dict[str, List[Tuple[str, float]]],\n",
    "                          reranker: Reranker,\n",
    "                          query_store: QueryStore,\n",
    "                          doc_store: DocumentStore) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Memory-efficient reranking of initial retrieval results\"\"\"\n",
    "    reranked_runs = {}\n",
    "    \n",
    "    for qid in tqdm(initial_runs):\n",
    "        query = query_store.get_query(qid)\n",
    "        all_reranked = []\n",
    "        \n",
    "        # Process documents in batches\n",
    "        doc_ids = [docid for docid, _ in initial_runs[qid]]\n",
    "        for batch_doc_ids in batch_iterator(doc_ids, reranker.batch_size):\n",
    "            doc_texts = doc_store.get_documents(batch_doc_ids)\n",
    "            batch_reranked = reranker.rerank_batch(query, doc_texts, batch_doc_ids)\n",
    "            all_reranked.extend(batch_reranked)\n",
    "        \n",
    "        # Sort by score\n",
    "        all_reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        reranked_runs[qid] = all_reranked\n",
    "    \n",
    "    return reranked_runs\n",
    "\n",
    "initial_runs = None\n",
    "doc_store = None\n",
    "query_store = None\n",
    "\n",
    "# Example usage for different models:\n",
    "def run_reranking_pipeline(model_name: str, \n",
    "                          initial_run_file: str,\n",
    "                          qrels_file: str,\n",
    "                          output_run_file: str,\n",
    "                          top_k: int = 1000):\n",
    "    # Load initial BM25 results\n",
    "    global initial_runs\n",
    "    if not initial_runs:\n",
    "        logging.info(f\"Loading inital BM25 results\")\n",
    "        initial_runs = get_bm25_run(top_k, initial_run_file)\n",
    "\n",
    "    # A bit of optimization\n",
    "    # due to memory constraints I cannot load all the documents\n",
    "    # instead I'll check which are referenced in BM25's rankings\n",
    "    # and tell DocumentStore to save only those.\n",
    "\n",
    "    doc_ids = {hit[0] for query_hits in initial_runs.values() for hit in query_hits}\n",
    "\n",
    "    # Initialize stores and reranker\n",
    "    global doc_store\n",
    "    global query_store\n",
    "    if not doc_store:\n",
    "        logging.info(\"Initializing document and query stores\")\n",
    "        doc_store = DocumentStore(\"fr\", doc_ids=doc_ids)\n",
    "        query_store = QueryStore(\"fr\")\n",
    "\n",
    "    # Initialize reranker\n",
    "    logging.info(f\"Initializing model\")\n",
    "    reranker = Reranker(model_name, batch_size=32)\n",
    "\n",
    "    # Rerank\n",
    "    logging.info(f\"Reranking queries...\")\n",
    "    reranked_runs = rerank_runs(initial_runs, reranker, query_store, doc_store)\n",
    "    \n",
    "    # Save results\n",
    "    logging.info(f\"Saving results...\")\n",
    "    save_runs_with_validation(reranked_runs, output_run_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import datasets\n",
    "from typing import Dict, List\n",
    "\n",
    "def evaluate_all_models(lang: str = 'fr', \n",
    "                       cache_dir: str = \"hf_datasets_cache\",\n",
    "                       output_dir: str = \"runs\"):\n",
    "    \"\"\"\n",
    "    Evaluates all models on MIRACL dataset\n",
    "    \"\"\"\n",
    "\n",
    "    topK = 1000\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(f'model_evaluation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Models to evaluate\n",
    "    models = [\n",
    "        \"facebook/mcontriever-msmarco\",\n",
    "        \"castorini/mdpr-tied-pft-msmarco\",\n",
    "        \"castorini/mdpr-tied-pft-msmarco-ft-miracl-fr\",\n",
    "        \"antoinelouis/crossencoder-camembert-base-mmarcoFR\"\n",
    "    ]\n",
    "\n",
    "    # Load initial BM25 runs\n",
    "    initial_run_file = f\"run.miracl.bm25.{lang}.dev.txt\"\n",
    "    qrels_file = f\"qrels.miracl-v1.0-{lang}-dev.tsv\"\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_name in models:\n",
    "        logging.info(f\"\\nEvaluating {model_name}\")\n",
    "        try:\n",
    "            # Generate run name\n",
    "            run_name = model_name.split('/')[-1]\n",
    "            output_run_file = os.path.join(output_dir, f\"run.miracl.{run_name}.{lang}.dev.txt\")\n",
    "\n",
    "            # Run evaluation\n",
    "            run_reranking_pipeline(\n",
    "                model_name=model_name,\n",
    "                initial_run_file=initial_run_file,\n",
    "                qrels_file=qrels_file,\n",
    "                output_run_file=output_run_file,\n",
    "                top_k=topK\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "\n",
    "    # Evaluate fine-tuned Camembert if available\n",
    "    try:\n",
    "        ft_model_path = \"output/crossencoder-camembert_miracl_fr-2024-11-11_15-55-59\"\n",
    "        if os.path.exists(ft_model_path):\n",
    "            logging.info(\"\\nEvaluating fine-tuned Camembert cross-encoder\")\n",
    "            \n",
    "            run_reranking_pipeline(\n",
    "                model_name=ft_model_path,\n",
    "                initial_run_file=initial_run_file,\n",
    "                qrels_file=qrels_file,\n",
    "                output_run_file=os.path.join(output_dir, f\"run.miracl.crossencoder-camembert-ft.{lang}.dev.txt\"),\n",
    "                top_k=topK\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating fine-tuned Camembert: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_all_models(\n",
    "    lang='fr',\n",
    "    cache_dir=\"hf_datasets_cache\",\n",
    "    output_dir=\"runs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.1370\n",
      "P@10\t0.0560\n",
      "AP@10\t0.1303\n",
      "nDCG@10\t0.1832\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv run.miracl.bm25.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.1603\n",
      "P@10\t0.0755\n",
      "AP@10\t0.1798\n",
      "nDCG@10\t0.2525\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv runs/run.miracl.mcontriever-msmarco.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.1370\n",
      "P@10\t0.0726\n",
      "AP@10\t0.1630\n",
      "nDCG@10\t0.2341\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv runs/run.miracl.mdpr-tied-pft-msmarco.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.1778\n",
      "P@10\t0.0866\n",
      "AP@10\t0.2052\n",
      "nDCG@10\t0.2855\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv runs/run.miracl.mdpr-tied-pft-msmarco-ft-miracl-fr.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.4169\n",
      "P@10\t0.1239\n",
      "AP@10\t0.3728\n",
      "nDCG@10\t0.4688\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv runs/run.miracl.crossencoder-camembert-base-mmarcoFR.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t0.4869\n",
      "P@10\t0.1327\n",
      "AP@10\t0.4349\n",
      "nDCG@10\t0.5245\n"
     ]
    }
   ],
   "source": [
    "!ir_measures qrels.miracl-v1.0-fr-dev.tsv runs/run.miracl.crossencoder-camembert-ft.fr.dev.txt 'P@1 P@10 AP@10 nDCG@10' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
