{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azatserikbayev/anaconda3/envs/pyserini/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from huggingface_hub import HfFolder, Repository, create_repo\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "def load_miracl_data(lang='fr'):\n",
    "    \"\"\"Load MIRACL dataset for given language\"\"\"\n",
    "    query_judgements = datasets.load_dataset(\n",
    "        \"miracl/miracl\", \n",
    "        lang, \n",
    "        split='train',\n",
    "        cache_dir=\"hf_datasets_cache\"\n",
    "    )\n",
    "    return query_judgements\n",
    "\n",
    "def get_cache_path(lang, num_negatives):\n",
    "    \"\"\"Generate cache file path\"\"\"\n",
    "    cache_dir = \"miracl_cache\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    return os.path.join(cache_dir, f\"train_samples_{lang}_neg{num_negatives}.pkl\")\n",
    "\n",
    "def save_to_cache(train_samples, cache_path):\n",
    "    \"\"\"Save processed samples to cache\"\"\"\n",
    "    logging.info(f\"Saving processed samples to {cache_path}\")\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(train_samples, f)\n",
    "\n",
    "def load_from_cache(cache_path):\n",
    "    \"\"\"Load processed samples from cache\"\"\"\n",
    "    logging.info(f\"Loading processed samples from {cache_path}\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_training_samples_with_hard_negatives(dataset, lang='fr', num_negatives=30, use_cache=True):\n",
    "    \"\"\"Create training samples with hard negative mining using BM25, with caching\"\"\"\n",
    "    cache_path = get_cache_path(lang, num_negatives)\n",
    "    \n",
    "    # Try to load from cache first\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        return load_from_cache(cache_path)\n",
    "    \n",
    "    logging.info(\"Cache not found or disabled. Creating new training samples...\")\n",
    "    \n",
    "    # Save intermediate results in case of runtime failure\n",
    "    intermediate_cache_path = cache_path + \".partial\"\n",
    "    intermediate_save_frequency = 1000  # Save every 1000 queries\n",
    "    \n",
    "    logging.info(\"Initializing BM25 searcher\")\n",
    "    searcher = LuceneSearcher.from_prebuilt_index(f'miracl-v1.0-{lang}')\n",
    "    searcher.set_language(lang)\n",
    "    \n",
    "    train_samples = []\n",
    "    \n",
    "    logging.info(\"Creating training samples with hard negatives\")\n",
    "    for idx, data in enumerate(tqdm(dataset)):\n",
    "        query = data['query']\n",
    "        positives = data['positive_passages']\n",
    "        negatives = data['negative_passages']\n",
    "        \n",
    "        # Get document IDs for filtering\n",
    "        positive_ids = [p['docid'] for p in positives]\n",
    "        negative_ids = [p['docid'] for p in negatives]\n",
    "        \n",
    "        # Search using BM25 to get hard negatives\n",
    "        hits = searcher.search(query, k=100)\n",
    "        bm25_negatives = []\n",
    "        for hit in hits:\n",
    "            info = json.loads(hit.lucene_document.get('raw'))\n",
    "            if info['docid'] not in positive_ids and info['docid'] not in negative_ids:\n",
    "                bm25_negatives.append(info)\n",
    "        \n",
    "        # Combine original negatives with BM25 negatives and shuffle\n",
    "        all_negatives = negatives + bm25_negatives\n",
    "        shuffle(all_negatives)\n",
    "        selected_negatives = all_negatives[:num_negatives]\n",
    "        \n",
    "        # Create positive examples\n",
    "        for pos in positives:\n",
    "            train_samples.append(\n",
    "                InputExample(\n",
    "                    texts=[query, pos['text']],\n",
    "                    label=1.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Create negative examples\n",
    "        for neg in selected_negatives:\n",
    "            train_samples.append(\n",
    "                InputExample(\n",
    "                    texts=[query, neg['text']],\n",
    "                    label=0.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if (idx + 1) % intermediate_save_frequency == 0:\n",
    "            logging.info(f\"Saving intermediate results after {idx + 1} queries...\")\n",
    "            save_to_cache(train_samples, intermediate_cache_path)\n",
    "    \n",
    "    # Save final results to cache\n",
    "    save_to_cache(train_samples, cache_path)\n",
    "    \n",
    "    # Clean up intermediate cache if it exists\n",
    "    if os.path.exists(intermediate_cache_path):\n",
    "        os.remove(intermediate_cache_path)\n",
    "    \n",
    "    return train_samples\n",
    "\n",
    "def push_to_hub(model_path, repo_name, username, lang):\n",
    "    \"\"\"Push the fine-tuned model to HuggingFace Hub\"\"\"\n",
    "    full_repo_name = f\"{username}/{repo_name}\"\n",
    "    \n",
    "    # Create repository\n",
    "    logging.info(f\"Creating repository: {full_repo_name}\")\n",
    "    create_repo(full_repo_name, exist_ok=True)\n",
    "    \n",
    "    # Create model card\n",
    "    model_card = f\"\"\"\n",
    "# MIRACL Cross-Encoder ({lang})\n",
    "\n",
    "This model is a fine-tuned version of [antoinelouis/crossencoder-camembert-base-mmarcoFR](https://huggingface.co/antoinelouis/crossencoder-camembert-base-mmarcoFR) on the MIRACL dataset for {lang} language. It uses hard negative mining with BM25 for better training data.\n",
    "\n",
    "## Training\n",
    "- The model was trained on MIRACL {lang} dataset\n",
    "- Hard negative mining was performed using BM25\n",
    "- For each query, we used all positive passages and up to 30 negative passages (combination of original negatives and BM25 hard negatives)\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\"{full_repo_name}\")\n",
    "scores = model.predict([[\"query\", \"document_text\"]])\n",
    "```\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(model_path, \"README.md\"), \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    # Clone the repo\n",
    "    repo = Repository(\n",
    "        local_dir=model_path,\n",
    "        clone_from=full_repo_name,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    # Push to hub\n",
    "    logging.info(\"Pushing model to HuggingFace Hub...\")\n",
    "    repo.push_to_hub()\n",
    "    \n",
    "    logging.info(f\"Model successfully pushed to: https://huggingface.co/{full_repo_name}\")\n",
    "\n",
    "def main():\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(f'fine_tuning_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "    lang = 'fr'  # language code\n",
    "    train_batch_size = 16\n",
    "    num_epochs = 2\n",
    "    num_negatives = 30  # number of negative examples per query\n",
    "    use_cache = True  # whether to use cached dataset\n",
    "    model_save_path = f\"output/training_miracl_{lang}-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # HuggingFace parameters\n",
    "    username = \"azat-serikbayev\"  # Replace with your HF username\n",
    "    repo_name = f\"crossencoder-camembert-miracl-{lang}\"\n",
    "\n",
    "    # Load MIRACL dataset\n",
    "    logging.info(\"Loading MIRACL dataset\")\n",
    "    dataset = load_miracl_data(lang=lang)\n",
    "    \n",
    "    # Create or load training samples with hard negatives\n",
    "    train_samples = create_training_samples_with_hard_negatives(\n",
    "        dataset,\n",
    "        lang=lang,\n",
    "        num_negatives=num_negatives,\n",
    "        use_cache=use_cache\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Working with {len(train_samples)} training samples\")\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_samples,\n",
    "        shuffle=True,\n",
    "        batch_size=train_batch_size\n",
    "    )\n",
    "    \n",
    "    # Calculate warmup steps\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "    logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "    # Load base model\n",
    "    model = CrossEncoder(\n",
    "        \"antoinelouis/crossencoder-camembert-base-mmarcoFR\",\n",
    "        num_labels=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    logging.info(\"Starting training\")\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        evaluator=None,\n",
    "        epochs=num_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=model_save_path\n",
    "    )\n",
    "\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "    # # Push model to HuggingFace Hub\n",
    "    # push_to_hub(model_save_path, repo_name, username, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 2289/2289 [38:41<00:00,  1.01s/it]\n",
      "Epoch: 100%|██████████| 1/1 [38:41<00:00, 2321.10s/it]\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
